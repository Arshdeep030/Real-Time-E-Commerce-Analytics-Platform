

# Real-Time E-Commerce Analytics Platform

An end-to-end **real-time data engineering project** that simulates e-commerce traffic, processes streaming events, applies analytics-ready transformations, and delivers **business-critical insights via dashboards**.

This project demonstrates **production-grade data engineering** using Kafka, Spark, Airflow, dbt, Redshift, and Grafana.

---

## What This Project Solves

Modern e-commerce platforms need:

* Real-time visibility into user behavior
* Reliable analytics pipelines
* Scalable, testable data models
* Business-ready dashboards for decision-making

This project builds exactly that — from **event generation** to **executive dashboards**.

---

## High-Level Architecture

```
Kafka (Event Stream)
   ↓
Spark (Sessionization & Aggregation)
   ↓
Amazon Redshift (Data Warehouse)
   ↓
dbt (Transformations & Tests)
   ↓
Airflow (Orchestration)
   ↓
Grafana (Analytics Dashboards)
```

---

## Tech Stack

| Layer             | Technology                          |
| ----------------- | ----------------------------------- |
| Event Streaming   | Apache Kafka                        |
| Stream Processing | Apache Spark (Structured Streaming) |
| Orchestration     | Apache Airflow                      |
| Data Warehouse    | Amazon Redshift                     |
| Transformations   | dbt                                 |
| Data Quality      | dbt Tests                           |
| Visualization     | Grafana                             |
| Infrastructure    | Docker & Docker Compose             |
| Languages         | SQL, Python                         |

---

## Project Structure

```
├── airflow/                    # Orchestration layer
│   ├── dags/
│   │   ├── redshift_analytics_dag.py
│   │   ├── redshift_load_dag.py
│   │   └── redshift_session_dag.py
│   └── sql/                    # SQL used by Airflow operators
│       ├── check_fact_user_sessions.sql
│       ├── dim_time.sql
│       ├── dim_users.sql
│       └── fact_user_sessions.sql
│
├── data_generator/             # Real-time traffic simulation
│   ├── producer.py             # Kafka producer
│   ├── requirements.txt
│   └── readme.md
│
├── dbt/                        # Transformation layer
│   ├── models/
│   │   ├── staging/
│   │   │   ├── schema.yml
│   │   │   └── stg_sessionized_events.sql
│   │   └── marts/
│   │       ├── dim_time.sql
│   │       ├── dim_users.sql
│   │       ├── fact_user_sessions.sql
│   │       └── fct_conversion_funnel.sql
│   ├── screenshots/
│   │   ├── dbt_lineage_medallion.png
│   │   └── dbt_tests_passed.png
│   ├── dbt_project.yml
│   └── profiles.yml
│
├── grafana/                    # Visualization layer
│   ├── queries/
│   │   ├── active_users.sql
│   │   ├── conversion_funnel.sql
│   │   ├── daily_revenue.sql
│   │   ├── funnel_efficiency.sql
│   │   └── total_revenue.sql
│   ├── screenshots/
│   │   ├── active_users_events.png
│   │   ├── conversion_funnel.png
│   │   ├── daily_revenue.png
│   │   ├── funnel_efficiency.png
│   │   └── total_revenue.png
│   └── readme.md
│
├── kafka/                      # Streaming infrastructure
│   ├── create_topics.sh
│   └── docker-compose.yml
│
├── spark/                      # Stream processing
│   ├── streaming/
│   │   ├── sessionization.py
│   │   └── schema.py
│   └── batch/
│       └── daily_aggregations.py
│
├── docker-compose.yml          # Full stack deployment
├── Dockerfile                  # Custom Airflow/Spark image
└── README.md
```

---

## Data Flow Explained

### Data Generation (Kafka)

* `data_generator/producer.py` simulates user activity:

  * page views
  * add-to-cart
  * purchases
* Events are pushed into Kafka topics.

---

### Stream Processing (Spark)

* **Sessionization** groups events by user sessions.
* Streaming jobs enrich raw events and write results to Redshift.
* Batch jobs compute daily aggregates.

---

### Orchestration (Airflow)

Airflow DAGs manage:

* Data loading into Redshift
* dbt model execution
* Data quality validation

Key DAG:

```bash
redshift_analytics_dag
```

---

### Transformation & Modeling (dbt)

**Staging Layer**

* Cleans raw sessionized events
* Standardizes event types and timestamps

**Mart Layer**

* `fact_user_sessions` – engagement & activity
* `fct_conversion_funnel` – revenue & conversion
* `dim_users` – user attributes
* `dim_time` – time intelligence

Data quality enforced via dbt tests:

* `not_null`
* `unique`
* Referential integrity

---

### Visualization (Grafana)

Dashboards include:

* Total Revenue
* Active Users
* Daily Revenue Trends
* Conversion Funnel
* Funnel Efficiency %

Queries are stored in:

```
grafana/queries/
```

Screenshots are provided as proof of execution.

---

## Example Business Insights

* Conversion Rate tracking
* Drop-off analysis (View → Cart → Purchase)
* Active user monitoring
* Day-over-day revenue growth

---

##  Documentation

dbt documentation generated with:

```bash
dbt docs generate
dbt docs serve
```

Includes:

* Full lineage graph
* Column-level metadata
* Model descriptions

---

##  Data Quality & Reliability

* Automated dbt tests
* Airflow task monitoring
* Idempotent transformations
* Modular, scalable architecture

---

##  Resume-Ready Highlights

* Built a **real-time analytics platform** using Kafka, Spark, Airflow, dbt, Redshift, and Grafana
* Modeled **conversion funnels** and **session-level analytics**
* Delivered **executive-grade dashboards** with live metrics
* Implemented **data quality testing and lineage documentation**

---

##  Future Enhancements

* SLA-based alerts via Slack
* CI/CD for dbt with GitHub Actions
* SCD Type-2 dimensions
* Cloud deployment on AWS ECS/EKS
* Data freshness checks

---

